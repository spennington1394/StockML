{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    # shuffle two arrays in the same way\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it \n",
    "            to False will split datasets in a random way\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "    # add date as a column\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
    "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    if split_by_date:\n",
    "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
    "        train_samples = int((1 - test_size) * len(X))\n",
    "        result[\"X_train\"] = X[:train_samples]\n",
    "        result[\"y_train\"] = y[:train_samples]\n",
    "        result[\"X_test\"]  = X[train_samples:]\n",
    "        result[\"y_test\"]  = y[train_samples:]\n",
    "        if shuffle:\n",
    "            # shuffle the datasets for training (if shuffle parameter is set)\n",
    "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "    else:    \n",
    "        # split the dataset randomly\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                test_size=test_size, shuffle=shuffle)\n",
    "    # get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # remove duplicated dates in the testing dataframe\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # remove dates from the training/testing sets & convert to float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 50\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 5\n",
    "# whether to scale feature columns & output price as well\n",
    "SCALE = True\n",
    "scale_str = f\"sc-{int(SCALE)}\"\n",
    "# whether to shuffle the dataset\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
    "# whether to split the training/testing set by date\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "### model parameters\n",
    "N_LAYERS = 3\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.2\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = True \n",
    "### training parameters\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "# Amazon stock market\n",
    "ticker = \"TSLA\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
    "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2110 samples, validate on 528 samples\n",
      "Epoch 1/100\n",
      "2080/2110 [============================>.] - ETA: 1s - loss: 0.0018 - mean_absolute_error: 0.0296\n",
      "Epoch 00001: val_loss improved from inf to 0.00048, saving model to results/2021-03-10_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-5-layers-3-units-256-b.h5\n",
      "2110/2110 [==============================] - 82s 39ms/sample - loss: 0.0018 - mean_absolute_error: 0.0294 - val_loss: 4.8462e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 2/100\n",
      "2080/2110 [============================>.] - ETA: 1s - loss: 4.0378e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00002: val_loss did not improve from 0.00048\n",
      "2110/2110 [==============================] - 75s 36ms/sample - loss: 4.0066e-04 - mean_absolute_error: 0.0128 - val_loss: 5.0939e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 3/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 6.0087e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 00003: val_loss did not improve from 0.00048\n",
      "2110/2110 [==============================] - 70s 33ms/sample - loss: 6.0666e-04 - mean_absolute_error: 0.0171 - val_loss: 0.0042 - val_mean_absolute_error: 0.0332\n",
      "Epoch 4/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 6.5390e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 00004: val_loss improved from 0.00048 to 0.00043, saving model to results/2021-03-10_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-5-layers-3-units-256-b.h5\n",
      "2110/2110 [==============================] - 72s 34ms/sample - loss: 6.5081e-04 - mean_absolute_error: 0.0182 - val_loss: 4.3368e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 5/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 4.5394e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00005: val_loss improved from 0.00043 to 0.00038, saving model to results/2021-03-10_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-5-layers-3-units-256-b.h5\n",
      "2110/2110 [==============================] - 70s 33ms/sample - loss: 4.5029e-04 - mean_absolute_error: 0.0142 - val_loss: 3.7617e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 6/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 4.2850e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00006: val_loss improved from 0.00038 to 0.00023, saving model to results/2021-03-10_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-5-layers-3-units-256-b.h5\n",
      "2110/2110 [==============================] - 68s 32ms/sample - loss: 4.2642e-04 - mean_absolute_error: 0.0154 - val_loss: 2.3048e-04 - val_mean_absolute_error: 0.0091\n",
      "Epoch 7/100\n",
      "2080/2110 [============================>.] - ETA: 1s - loss: 2.9709e-04 - mean_absolute_error: 0.0116\n",
      "Epoch 00007: val_loss improved from 0.00023 to 0.00022, saving model to results/2021-03-10_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-5-layers-3-units-256-b.h5\n",
      "2110/2110 [==============================] - 77s 36ms/sample - loss: 2.9557e-04 - mean_absolute_error: 0.0116 - val_loss: 2.1667e-04 - val_mean_absolute_error: 0.0093\n",
      "Epoch 8/100\n",
      "2080/2110 [============================>.] - ETA: 1s - loss: 3.0352e-04 - mean_absolute_error: 0.0116\n",
      "Epoch 00008: val_loss did not improve from 0.00022\n",
      "2110/2110 [==============================] - 77s 37ms/sample - loss: 3.0052e-04 - mean_absolute_error: 0.0115 - val_loss: 2.7972e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 9/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.8056e-04 - mean_absolute_error: 0.0115\n",
      "Epoch 00009: val_loss did not improve from 0.00022\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 2.7720e-04 - mean_absolute_error: 0.0114 - val_loss: 4.1994e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 10/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.7460e-04 - mean_absolute_error: 0.0109\n",
      "Epoch 00010: val_loss did not improve from 0.00022\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 2.7101e-04 - mean_absolute_error: 0.0108 - val_loss: 2.9920e-04 - val_mean_absolute_error: 0.0086\n",
      "Epoch 11/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.5995e-04 - mean_absolute_error: 0.0105\n",
      "Epoch 00011: val_loss did not improve from 0.00022\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 2.6142e-04 - mean_absolute_error: 0.0105 - val_loss: 2.6360e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 12/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.7163e-04 - mean_absolute_error: 0.0112\n",
      "Epoch 00012: val_loss did not improve from 0.00022\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 2.7001e-04 - mean_absolute_error: 0.0112 - val_loss: 2.2358e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 13/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 3.7003e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00013: val_loss improved from 0.00022 to 0.00020, saving model to results/2021-03-10_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-5-layers-3-units-256-b.h5\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 3.6510e-04 - mean_absolute_error: 0.0131 - val_loss: 2.0097e-04 - val_mean_absolute_error: 0.0087\n",
      "Epoch 14/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.6358e-04 - mean_absolute_error: 0.0116\n",
      "Epoch 00014: val_loss did not improve from 0.00020\n",
      "2110/2110 [==============================] - 65s 31ms/sample - loss: 2.6119e-04 - mean_absolute_error: 0.0115 - val_loss: 3.2472e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 15/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 3.3987e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00015: val_loss did not improve from 0.00020\n",
      "2110/2110 [==============================] - 64s 30ms/sample - loss: 3.3702e-04 - mean_absolute_error: 0.0132 - val_loss: 2.7046e-04 - val_mean_absolute_error: 0.0149\n",
      "Epoch 16/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.2518e-04 - mean_absolute_error: 0.0108\n",
      "Epoch 00016: val_loss did not improve from 0.00020\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 2.2284e-04 - mean_absolute_error: 0.0108 - val_loss: 2.0813e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 17/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.3618e-04 - mean_absolute_error: 0.0098\n",
      "Epoch 00017: val_loss did not improve from 0.00020\n",
      "2110/2110 [==============================] - 68s 32ms/sample - loss: 2.3723e-04 - mean_absolute_error: 0.0098 - val_loss: 4.4686e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 18/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.9947e-04 - mean_absolute_error: 0.0095\n",
      "Epoch 00018: val_loss did not improve from 0.00020\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 1.9799e-04 - mean_absolute_error: 0.0095 - val_loss: 3.2929e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 19/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.9592e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00019: val_loss did not improve from 0.00020\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 2.9259e-04 - mean_absolute_error: 0.0124 - val_loss: 2.0288e-04 - val_mean_absolute_error: 0.0090\n",
      "Epoch 20/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.5962e-04 - mean_absolute_error: 0.0102\n",
      "Epoch 00020: val_loss did not improve from 0.00020\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 2.5734e-04 - mean_absolute_error: 0.0103 - val_loss: 5.3116e-04 - val_mean_absolute_error: 0.0201\n",
      "Epoch 21/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.2690e-04 - mean_absolute_error: 0.0105\n",
      "Epoch 00021: val_loss did not improve from 0.00020\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 2.2540e-04 - mean_absolute_error: 0.0105 - val_loss: 2.0780e-04 - val_mean_absolute_error: 0.0076\n",
      "Epoch 22/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.7155e-04 - mean_absolute_error: 0.0088\n",
      "Epoch 00022: val_loss did not improve from 0.00020\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 1.6941e-04 - mean_absolute_error: 0.0088 - val_loss: 2.0849e-04 - val_mean_absolute_error: 0.0078\n",
      "Epoch 23/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.0411e-04 - mean_absolute_error: 0.0092\n",
      "Epoch 00023: val_loss did not improve from 0.00020\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 2.0150e-04 - mean_absolute_error: 0.0091 - val_loss: 2.2711e-04 - val_mean_absolute_error: 0.0089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.9333e-04 - mean_absolute_error: 0.0095\n",
      "Epoch 00024: val_loss improved from 0.00020 to 0.00020, saving model to results/2021-03-10_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-5-layers-3-units-256-b.h5\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 1.9307e-04 - mean_absolute_error: 0.0094 - val_loss: 1.9909e-04 - val_mean_absolute_error: 0.0076\n",
      "Epoch 25/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.0064e-04 - mean_absolute_error: 0.0094\n",
      "Epoch 00025: val_loss did not improve from 0.00020\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 1.9840e-04 - mean_absolute_error: 0.0094 - val_loss: 2.0819e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 26/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.7385e-04 - mean_absolute_error: 0.0113\n",
      "Epoch 00026: val_loss did not improve from 0.00020\n",
      "2110/2110 [==============================] - 62s 29ms/sample - loss: 2.7201e-04 - mean_absolute_error: 0.0112 - val_loss: 2.0583e-04 - val_mean_absolute_error: 0.0085\n",
      "Epoch 27/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.0603e-04 - mean_absolute_error: 0.0099\n",
      "Epoch 00027: val_loss did not improve from 0.00020\n",
      "2110/2110 [==============================] - 62s 29ms/sample - loss: 2.0397e-04 - mean_absolute_error: 0.0098 - val_loss: 2.0431e-04 - val_mean_absolute_error: 0.0091\n",
      "Epoch 28/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 3.1368e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00028: val_loss did not improve from 0.00020\n",
      "2110/2110 [==============================] - 62s 29ms/sample - loss: 3.1249e-04 - mean_absolute_error: 0.0123 - val_loss: 2.2396e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 29/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.4190e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00029: val_loss improved from 0.00020 to 0.00019, saving model to results/2021-03-10_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-5-layers-3-units-256-b.h5\n",
      "2110/2110 [==============================] - 62s 29ms/sample - loss: 2.3998e-04 - mean_absolute_error: 0.0119 - val_loss: 1.8956e-04 - val_mean_absolute_error: 0.0071\n",
      "Epoch 30/100\n",
      "2080/2110 [============================>.] - ETA: 1s - loss: 1.9457e-04 - mean_absolute_error: 0.0106\n",
      "Epoch 00030: val_loss did not improve from 0.00019\n",
      "2110/2110 [==============================] - 76s 36ms/sample - loss: 1.9661e-04 - mean_absolute_error: 0.0106 - val_loss: 1.9138e-04 - val_mean_absolute_error: 0.0076\n",
      "Epoch 31/100\n",
      "2080/2110 [============================>.] - ETA: 4s - loss: 2.8071e-04 - mean_absolute_error: 0.0113\n",
      "Epoch 00031: val_loss did not improve from 0.00019\n",
      "2110/2110 [==============================] - 289s 137ms/sample - loss: 2.7702e-04 - mean_absolute_error: 0.0112 - val_loss: 2.3651e-04 - val_mean_absolute_error: 0.0092\n",
      "Epoch 32/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.0409e-04 - mean_absolute_error: 0.0093\n",
      "Epoch 00032: val_loss did not improve from 0.00019\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 2.0337e-04 - mean_absolute_error: 0.0093 - val_loss: 2.1979e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 33/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.0863e-04 - mean_absolute_error: 0.0099\n",
      "Epoch 00033: val_loss did not improve from 0.00019\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 2.0791e-04 - mean_absolute_error: 0.0099 - val_loss: 2.7152e-04 - val_mean_absolute_error: 0.0085\n",
      "Epoch 34/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.0527e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 00034: val_loss did not improve from 0.00019\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 2.0603e-04 - mean_absolute_error: 0.0087 - val_loss: 2.9285e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 35/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.7115e-04 - mean_absolute_error: 0.0095\n",
      "Epoch 00035: val_loss did not improve from 0.00019\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 1.8063e-04 - mean_absolute_error: 0.0096 - val_loss: 2.4089e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 36/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.2395e-04 - mean_absolute_error: 0.0100\n",
      "Epoch 00036: val_loss did not improve from 0.00019\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 2.2212e-04 - mean_absolute_error: 0.0099 - val_loss: 1.9152e-04 - val_mean_absolute_error: 0.0080\n",
      "Epoch 37/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.2677e-04 - mean_absolute_error: 0.0103\n",
      "Epoch 00037: val_loss did not improve from 0.00019\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 2.2712e-04 - mean_absolute_error: 0.0103 - val_loss: 2.6146e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 38/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.0285e-04 - mean_absolute_error: 0.0093\n",
      "Epoch 00038: val_loss did not improve from 0.00019\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 2.0067e-04 - mean_absolute_error: 0.0093 - val_loss: 1.9079e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 39/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.9742e-04 - mean_absolute_error: 0.0101\n",
      "Epoch 00039: val_loss did not improve from 0.00019\n",
      "2110/2110 [==============================] - 63s 30ms/sample - loss: 1.9832e-04 - mean_absolute_error: 0.0102 - val_loss: 4.7229e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 40/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.1085e-04 - mean_absolute_error: 0.0102\n",
      "Epoch 00040: val_loss did not improve from 0.00019\n",
      "2110/2110 [==============================] - 64s 30ms/sample - loss: 2.0993e-04 - mean_absolute_error: 0.0102 - val_loss: 2.7675e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 41/100\n",
      "2080/2110 [============================>.] - ETA: 1s - loss: 2.0675e-04 - mean_absolute_error: 0.0099\n",
      "Epoch 00041: val_loss improved from 0.00019 to 0.00018, saving model to results/2021-03-10_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-5-layers-3-units-256-b.h5\n",
      "2110/2110 [==============================] - 75s 36ms/sample - loss: 2.0979e-04 - mean_absolute_error: 0.0099 - val_loss: 1.8452e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 42/100\n",
      "2080/2110 [============================>.] - ETA: 3s - loss: 1.8449e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00042: val_loss did not improve from 0.00018\n",
      "2110/2110 [==============================] - 275s 131ms/sample - loss: 1.8220e-04 - mean_absolute_error: 0.0090 - val_loss: 1.9663e-04 - val_mean_absolute_error: 0.0082\n",
      "Epoch 43/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.6994e-04 - mean_absolute_error: 0.0091\n",
      "Epoch 00043: val_loss improved from 0.00018 to 0.00016, saving model to results/2021-03-10_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-5-layers-3-units-256-b.h5\n",
      "2110/2110 [==============================] - 71s 34ms/sample - loss: 1.7122e-04 - mean_absolute_error: 0.0091 - val_loss: 1.5965e-04 - val_mean_absolute_error: 0.0076\n",
      "Epoch 44/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.3951e-04 - mean_absolute_error: 0.0104\n",
      "Epoch 00044: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 68s 32ms/sample - loss: 2.3684e-04 - mean_absolute_error: 0.0103 - val_loss: 2.1680e-04 - val_mean_absolute_error: 0.0075\n",
      "Epoch 45/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.1790e-04 - mean_absolute_error: 0.0104\n",
      "Epoch 00045: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 70s 33ms/sample - loss: 2.1492e-04 - mean_absolute_error: 0.0103 - val_loss: 1.8862e-04 - val_mean_absolute_error: 0.0073\n",
      "Epoch 46/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.0072e-04 - mean_absolute_error: 0.0096\n",
      "Epoch 00046: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 69s 33ms/sample - loss: 2.0133e-04 - mean_absolute_error: 0.0096 - val_loss: 1.9888e-04 - val_mean_absolute_error: 0.0077\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.4279e-04 - mean_absolute_error: 0.0112\n",
      "Epoch 00047: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 68s 32ms/sample - loss: 2.4574e-04 - mean_absolute_error: 0.0113 - val_loss: 5.1334e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 48/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.1379e-04 - mean_absolute_error: 0.0103\n",
      "Epoch 00048: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 68s 32ms/sample - loss: 2.1140e-04 - mean_absolute_error: 0.0103 - val_loss: 2.1467e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 49/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.0521e-04 - mean_absolute_error: 0.0098\n",
      "Epoch 00049: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 73s 35ms/sample - loss: 2.0274e-04 - mean_absolute_error: 0.0097 - val_loss: 2.0853e-04 - val_mean_absolute_error: 0.0074\n",
      "Epoch 50/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.7876e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00050: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 70s 33ms/sample - loss: 1.7743e-04 - mean_absolute_error: 0.0090 - val_loss: 1.7912e-04 - val_mean_absolute_error: 0.0089\n",
      "Epoch 51/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.9941e-04 - mean_absolute_error: 0.0100\n",
      "Epoch 00051: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.9914e-04 - mean_absolute_error: 0.0100 - val_loss: 2.6457e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 52/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.5802e-04 - mean_absolute_error: 0.0092\n",
      "Epoch 00052: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.5596e-04 - mean_absolute_error: 0.0092 - val_loss: 1.7735e-04 - val_mean_absolute_error: 0.0076\n",
      "Epoch 53/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.8774e-04 - mean_absolute_error: 0.0097\n",
      "Epoch 00053: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.8653e-04 - mean_absolute_error: 0.0097 - val_loss: 2.0388e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 54/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.1750e-04 - mean_absolute_error: 0.0102\n",
      "Epoch 00054: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 2.1485e-04 - mean_absolute_error: 0.0101 - val_loss: 2.1654e-04 - val_mean_absolute_error: 0.0097\n",
      "Epoch 55/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.2166e-04 - mean_absolute_error: 0.0101\n",
      "Epoch 00055: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 2.1994e-04 - mean_absolute_error: 0.0101 - val_loss: 1.7108e-04 - val_mean_absolute_error: 0.0073\n",
      "Epoch 56/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.3862e-04 - mean_absolute_error: 0.0108\n",
      "Epoch 00056: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 2.3814e-04 - mean_absolute_error: 0.0108 - val_loss: 1.7681e-04 - val_mean_absolute_error: 0.0088\n",
      "Epoch 57/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.1895e-04 - mean_absolute_error: 0.0106\n",
      "Epoch 00057: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 66s 32ms/sample - loss: 2.1617e-04 - mean_absolute_error: 0.0106 - val_loss: 1.9012e-04 - val_mean_absolute_error: 0.0074\n",
      "Epoch 58/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.6986e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00058: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.6837e-04 - mean_absolute_error: 0.0082 - val_loss: 1.7246e-04 - val_mean_absolute_error: 0.0086\n",
      "Epoch 59/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.0708e-04 - mean_absolute_error: 0.0099\n",
      "Epoch 00059: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 66s 32ms/sample - loss: 2.0494e-04 - mean_absolute_error: 0.0098 - val_loss: 1.7915e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 60/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.7779e-04 - mean_absolute_error: 0.0092\n",
      "Epoch 00060: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.7576e-04 - mean_absolute_error: 0.0092 - val_loss: 1.6626e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 61/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.6516e-04 - mean_absolute_error: 0.0107\n",
      "Epoch 00061: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 2.6181e-04 - mean_absolute_error: 0.0107 - val_loss: 1.8632e-04 - val_mean_absolute_error: 0.0078\n",
      "Epoch 62/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.3164e-04 - mean_absolute_error: 0.0100\n",
      "Epoch 00062: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 2.2941e-04 - mean_absolute_error: 0.0100 - val_loss: 2.5883e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 63/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.9513e-04 - mean_absolute_error: 0.0089\n",
      "Epoch 00063: val_loss improved from 0.00016 to 0.00016, saving model to results/2021-03-10_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-5-layers-3-units-256-b.h5\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.9257e-04 - mean_absolute_error: 0.0088 - val_loss: 1.5688e-04 - val_mean_absolute_error: 0.0074\n",
      "Epoch 64/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.4901e-04 - mean_absolute_error: 0.0086\n",
      "Epoch 00064: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 1.5190e-04 - mean_absolute_error: 0.0087 - val_loss: 2.0233e-04 - val_mean_absolute_error: 0.0074\n",
      "Epoch 65/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.6006e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00065: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 1.6477e-04 - mean_absolute_error: 0.0091 - val_loss: 1.8430e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 66/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.3152e-04 - mean_absolute_error: 0.0107\n",
      "Epoch 00066: val_loss did not improve from 0.00016\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 2.2864e-04 - mean_absolute_error: 0.0107 - val_loss: 1.7396e-04 - val_mean_absolute_error: 0.0075\n",
      "Epoch 67/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.8430e-04 - mean_absolute_error: 0.0091\n",
      "Epoch 00067: val_loss improved from 0.00016 to 0.00015, saving model to results/2021-03-10_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-5-layers-3-units-256-b.h5\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 1.8494e-04 - mean_absolute_error: 0.0090 - val_loss: 1.5174e-04 - val_mean_absolute_error: 0.0067\n",
      "Epoch 68/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.7313e-04 - mean_absolute_error: 0.0089\n",
      "Epoch 00068: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 1.7219e-04 - mean_absolute_error: 0.0089 - val_loss: 2.6963e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 69/100\n",
      "2080/2110 [============================>.] - ETA: 1s - loss: 1.7733e-04 - mean_absolute_error: 0.0092\n",
      "Epoch 00069: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 78s 37ms/sample - loss: 1.7514e-04 - mean_absolute_error: 0.0092 - val_loss: 1.5707e-04 - val_mean_absolute_error: 0.0076\n",
      "Epoch 70/100\n",
      "2080/2110 [============================>.] - ETA: 9s - loss: 2.7516e-04 - mean_absolute_error: 0.0115 \n",
      "Epoch 00070: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 695s 329ms/sample - loss: 2.7273e-04 - mean_absolute_error: 0.0114 - val_loss: 6.8969e-04 - val_mean_absolute_error: 0.0170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.6657e-04 - mean_absolute_error: 0.0114\n",
      "Epoch 00071: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 2.6366e-04 - mean_absolute_error: 0.0113 - val_loss: 2.5675e-04 - val_mean_absolute_error: 0.0080\n",
      "Epoch 72/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.7527e-04 - mean_absolute_error: 0.0096\n",
      "Epoch 00072: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.7426e-04 - mean_absolute_error: 0.0095 - val_loss: 1.6747e-04 - val_mean_absolute_error: 0.0079\n",
      "Epoch 73/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.5779e-04 - mean_absolute_error: 0.0085\n",
      "Epoch 00073: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 1.5712e-04 - mean_absolute_error: 0.0085 - val_loss: 2.1375e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 74/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.8350e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00074: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.8158e-04 - mean_absolute_error: 0.0090 - val_loss: 1.8909e-04 - val_mean_absolute_error: 0.0081\n",
      "Epoch 75/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.7340e-04 - mean_absolute_error: 0.0093\n",
      "Epoch 00075: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 1.7162e-04 - mean_absolute_error: 0.0093 - val_loss: 1.6063e-04 - val_mean_absolute_error: 0.0080\n",
      "Epoch 76/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.9356e-04 - mean_absolute_error: 0.0091\n",
      "Epoch 00076: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 1.9638e-04 - mean_absolute_error: 0.0091 - val_loss: 1.6754e-04 - val_mean_absolute_error: 0.0080\n",
      "Epoch 77/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.7410e-04 - mean_absolute_error: 0.0094\n",
      "Epoch 00077: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 1.7216e-04 - mean_absolute_error: 0.0093 - val_loss: 2.2059e-04 - val_mean_absolute_error: 0.0074\n",
      "Epoch 78/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.7849e-04 - mean_absolute_error: 0.0092\n",
      "Epoch 00078: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.7644e-04 - mean_absolute_error: 0.0091 - val_loss: 2.9929e-04 - val_mean_absolute_error: 0.0097\n",
      "Epoch 79/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 3.0994e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00079: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 3.1379e-04 - mean_absolute_error: 0.0126 - val_loss: 5.0558e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 80/100\n",
      "2080/2110 [============================>.] - ETA: 1s - loss: 2.0509e-04 - mean_absolute_error: 0.0101\n",
      "Epoch 00080: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 120s 57ms/sample - loss: 2.0240e-04 - mean_absolute_error: 0.0100 - val_loss: 2.9426e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 81/100\n",
      "2080/2110 [============================>.] - ETA: 3s - loss: 1.9003e-04 - mean_absolute_error: 0.0102\n",
      "Epoch 00081: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 231s 110ms/sample - loss: 1.9088e-04 - mean_absolute_error: 0.0102 - val_loss: 1.9490e-04 - val_mean_absolute_error: 0.0076\n",
      "Epoch 82/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.4943e-04 - mean_absolute_error: 0.0080\n",
      "Epoch 00082: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 1.4873e-04 - mean_absolute_error: 0.0080 - val_loss: 1.7289e-04 - val_mean_absolute_error: 0.0080\n",
      "Epoch 83/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.9705e-04 - mean_absolute_error: 0.0097\n",
      "Epoch 00083: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 1.9538e-04 - mean_absolute_error: 0.0097 - val_loss: 1.7795e-04 - val_mean_absolute_error: 0.0070\n",
      "Epoch 84/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.4092e-04 - mean_absolute_error: 0.0085\n",
      "Epoch 00084: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.4000e-04 - mean_absolute_error: 0.0085 - val_loss: 1.6592e-04 - val_mean_absolute_error: 0.0069\n",
      "Epoch 85/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.5767e-04 - mean_absolute_error: 0.0084\n",
      "Epoch 00085: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.6354e-04 - mean_absolute_error: 0.0085 - val_loss: 3.1025e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 86/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.0903e-04 - mean_absolute_error: 0.0098\n",
      "Epoch 00086: val_loss improved from 0.00015 to 0.00015, saving model to results/2021-03-10_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-5-layers-3-units-256-b.h5\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 2.0641e-04 - mean_absolute_error: 0.0098 - val_loss: 1.5133e-04 - val_mean_absolute_error: 0.0070\n",
      "Epoch 87/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.6870e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00087: val_loss improved from 0.00015 to 0.00015, saving model to results/2021-03-10_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-5-layers-3-units-256-b.h5\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.6987e-04 - mean_absolute_error: 0.0091 - val_loss: 1.4744e-04 - val_mean_absolute_error: 0.0070\n",
      "Epoch 88/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.6701e-04 - mean_absolute_error: 0.0094\n",
      "Epoch 00088: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 1.6538e-04 - mean_absolute_error: 0.0093 - val_loss: 3.7394e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 89/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.2795e-04 - mean_absolute_error: 0.0109\n",
      "Epoch 00089: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 2.3420e-04 - mean_absolute_error: 0.0110 - val_loss: 1.7972e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 90/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.7600e-04 - mean_absolute_error: 0.0093\n",
      "Epoch 00090: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.7384e-04 - mean_absolute_error: 0.0092 - val_loss: 1.6134e-04 - val_mean_absolute_error: 0.0085\n",
      "Epoch 91/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.5965e-04 - mean_absolute_error: 0.0088\n",
      "Epoch 00091: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 1.5789e-04 - mean_absolute_error: 0.0087 - val_loss: 1.5467e-04 - val_mean_absolute_error: 0.0066\n",
      "Epoch 92/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.7408e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 00092: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 1.7223e-04 - mean_absolute_error: 0.0086 - val_loss: 1.4769e-04 - val_mean_absolute_error: 0.0070\n",
      "Epoch 93/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.8312e-04 - mean_absolute_error: 0.0094\n",
      "Epoch 00093: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.8182e-04 - mean_absolute_error: 0.0094 - val_loss: 1.7273e-04 - val_mean_absolute_error: 0.0091\n",
      "Epoch 94/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.7601e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 00094: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.7728e-04 - mean_absolute_error: 0.0088 - val_loss: 1.8174e-04 - val_mean_absolute_error: 0.0125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.5643e-04 - mean_absolute_error: 0.0084\n",
      "Epoch 00095: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.5799e-04 - mean_absolute_error: 0.0084 - val_loss: 1.9182e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 96/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.8118e-04 - mean_absolute_error: 0.0097\n",
      "Epoch 00096: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.8342e-04 - mean_absolute_error: 0.0097 - val_loss: 1.9430e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 97/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.5407e-04 - mean_absolute_error: 0.0088\n",
      "Epoch 00097: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.5557e-04 - mean_absolute_error: 0.0089 - val_loss: 2.1416e-04 - val_mean_absolute_error: 0.0074\n",
      "Epoch 98/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.9263e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00098: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 66s 31ms/sample - loss: 1.9017e-04 - mean_absolute_error: 0.0090 - val_loss: 1.7682e-04 - val_mean_absolute_error: 0.0076\n",
      "Epoch 99/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 2.4102e-04 - mean_absolute_error: 0.0104\n",
      "Epoch 00099: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 2.3827e-04 - mean_absolute_error: 0.0103 - val_loss: 1.5814e-04 - val_mean_absolute_error: 0.0091\n",
      "Epoch 100/100\n",
      "2080/2110 [============================>.] - ETA: 0s - loss: 1.7970e-04 - mean_absolute_error: 0.0091\n",
      "Epoch 00100: val_loss did not improve from 0.00015\n",
      "2110/2110 [==============================] - 67s 32ms/sample - loss: 1.7960e-04 - mean_absolute_error: 0.0091 - val_loss: 2.3396e-04 - val_mean_absolute_error: 0.0081\n"
     ]
    }
   ],
   "source": [
    "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# train the model and save the weights whenever we see \n",
    "# a new optimal model using ModelCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-70-87357480c861>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-70-87357480c861>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tensorboard --logdir=\"logs\"\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "tensorboard --logdir=\"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph(test_df):\n",
    "    \"\"\"\n",
    "    This function plots true close price along with predicted close price\n",
    "    with blue and red colors respectively\n",
    "    \"\"\"\n",
    "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
    "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(model, data):\n",
    "    \"\"\"\n",
    "    This function takes the `model` and `data` dict to \n",
    "    construct a final dataframe that includes the features along \n",
    "    with true and predicted prices of the testing dataset\n",
    "    \"\"\"\n",
    "    # if predicted future price is higher than the current, \n",
    "    # then calculate the true future price minus the current price, to get the buy profit\n",
    "    buy_profit  = lambda current, true_future, pred_future: true_future - current if pred_future > current else 0\n",
    "    # if the predicted future price is lower than the current price,\n",
    "    # then subtract the true future price from the current price\n",
    "    sell_profit = lambda current, true_future, pred_future: current - true_future if pred_future < current else 0\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    # perform prediction and get prices\n",
    "    y_pred = model.predict(X_test)\n",
    "    if SCALE:\n",
    "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    test_df = data[\"test_df\"]\n",
    "    # add predicted future prices to the dataframe\n",
    "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
    "    # add true future prices to the dataframe\n",
    "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
    "    # sort the dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    # add the buy profit column\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    # add the sell profit column\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    if SCALE:\n",
    "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "if SCALE:\n",
    "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
    "else:\n",
    "    mean_absolute_error = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final dataframe for the testing set\n",
    "final_df = get_final_df(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the accuracy by counting the number of positive profits\n",
    "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
    "# calculating total buy & sell profit\n",
    "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
    "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
    "# total profit by adding sell & buy together\n",
    "total_profit = total_buy_profit + total_sell_profit\n",
    "# dividing total profit by number of testing samples (number of trades)\n",
    "profit_per_trade = total_profit / len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 5 days is 607.03$\n",
      "huber_loss loss: 0.00014743709052157956\n",
      "Mean Absolute Error: 9.318549191369085\n",
      "Accuracy score: 0.5340909090909091\n",
      "Total buy profit: 1000.9233440160751\n",
      "Total sell profit: -462.1525661945343\n",
      "Total profit: 538.7707778215408\n",
      "Profit per trade: 1.020399200419585\n"
     ]
    }
   ],
   "source": [
    "# printing metrics\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Total buy profit:\", total_buy_profit)\n",
    "print(\"Total sell profit:\", total_sell_profit)\n",
    "print(\"Total profit:\", total_profit)\n",
    "print(\"Profit per trade:\", profit_per_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graph(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  open        high         low       close    adjclose  \\\n",
      "2020-12-31  699.989990  718.719971  691.119995  705.669983  705.669983   \n",
      "2021-01-07  777.630005  816.989990  775.200012  816.039978  816.039978   \n",
      "2021-01-12  831.000000  868.000000  827.340027  849.440002  849.440002   \n",
      "2021-01-20  858.739990  859.500000  837.280029  850.450012  850.450012   \n",
      "2021-01-22  834.309998  848.000000  828.619995  846.640015  846.640015   \n",
      "2021-01-29  830.000000  842.409973  780.099976  793.530029  793.530029   \n",
      "2021-02-05  845.000000  864.770020  838.969971  852.229980  852.229980   \n",
      "2021-02-10  843.640015  844.820007  800.020020  804.820007  804.820007   \n",
      "2021-02-16  818.000000  821.000000  792.440002  796.219971  796.219971   \n",
      "2021-02-25  726.150024  737.210022  670.580017  682.219971  682.219971   \n",
      "\n",
      "              volume ticker  adjclose_5  true_adjclose_5  buy_profit  \\\n",
      "2020-12-31  49649900   TSLA  754.039490       880.020020   48.369507   \n",
      "2021-01-07  51498900   TSLA  836.212891       845.000000   20.172913   \n",
      "2021-01-12  46270700   TSLA  867.204895       850.450012   17.764893   \n",
      "2021-01-20  25665900   TSLA  865.310486       864.159973   14.860474   \n",
      "2021-01-22  20066500   TSLA  858.179871       793.530029    0.000000   \n",
      "2021-01-29  34990800   TSLA  821.691833       852.229980   28.161804   \n",
      "2021-02-05  18524800   TSLA  826.691223       816.119995    0.000000   \n",
      "2021-02-10  36216100   TSLA  809.642822       787.380005    0.000000   \n",
      "2021-02-16  19802300   TSLA  775.710144       698.840027    0.000000   \n",
      "2021-02-25  39023900   TSLA  676.414551       621.440002    0.000000   \n",
      "\n",
      "            sell_profit  \n",
      "2020-12-31     0.000000  \n",
      "2021-01-07     0.000000  \n",
      "2021-01-12     0.000000  \n",
      "2021-01-20     0.000000  \n",
      "2021-01-22   -11.539856  \n",
      "2021-01-29     0.000000  \n",
      "2021-02-05    25.538757  \n",
      "2021-02-10    -4.822815  \n",
      "2021-02-16    20.509827  \n",
      "2021-02-25     5.805420  \n"
     ]
    }
   ],
   "source": [
    "print(final_df.tail(10))\n",
    "# save the final dataframe to csv-results folder\n",
    "csv_results_folder = \"csv-results\"\n",
    "if not os.path.isdir(csv_results_folder):\n",
    "    os.mkdir(csv_results_folder)\n",
    "csv_filename = os.path.join(csv_results_folder, model_name + \".csv\")\n",
    "final_df.to_csv(csv_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle _thread.RLock objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-87b8553faa95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_model.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
     ]
    }
   ],
   "source": [
    "pickle.dump(model, open('test_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('saved_figure.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
