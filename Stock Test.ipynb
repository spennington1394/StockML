{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kscomputer/opt/anaconda3/envs/PythonDataUPenn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    # shuffle two arrays in the same way\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it \n",
    "            to False will split datasets in a random way\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "    # add date as a column\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
    "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    if split_by_date:\n",
    "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
    "        train_samples = int((1 - test_size) * len(X))\n",
    "        result[\"X_train\"] = X[:train_samples]\n",
    "        result[\"y_train\"] = y[:train_samples]\n",
    "        result[\"X_test\"]  = X[train_samples:]\n",
    "        result[\"y_test\"]  = y[train_samples:]\n",
    "        if shuffle:\n",
    "            # shuffle the datasets for training (if shuffle parameter is set)\n",
    "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "    else:    \n",
    "        # split the dataset randomly\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                test_size=test_size, shuffle=shuffle)\n",
    "    # get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # remove duplicated dates in the testing dataframe\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # remove dates from the training/testing sets & convert to float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 50\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 1\n",
    "# whether to scale feature columns & output price as well\n",
    "SCALE = True\n",
    "scale_str = f\"sc-{int(SCALE)}\"\n",
    "# whether to shuffle the dataset\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
    "# whether to split the training/testing set by date\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "### model parameters\n",
    "N_LAYERS = 2\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "### training parameters\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "# Amazon stock market\n",
    "ticker = \"TSLA\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
    "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2110 samples, validate on 528 samples\n",
      "Epoch 1/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 0.0013 - mean_absolute_error: 0.0213\n",
      "Epoch 00001: val_loss improved from inf to 0.00021, saving model to results/2021-03-03_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-1-layers-2-units-256.h5\n",
      "2110/2110 [==============================] - 18s 9ms/sample - loss: 0.0013 - mean_absolute_error: 0.0211 - val_loss: 2.1288e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 2/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 3.0878e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00002: val_loss improved from 0.00021 to 0.00012, saving model to results/2021-03-03_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-1-layers-2-units-256.h5\n",
      "2110/2110 [==============================] - 16s 8ms/sample - loss: 3.1083e-04 - mean_absolute_error: 0.0129 - val_loss: 1.2458e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 3/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.2820e-04 - mean_absolute_error: 0.0102\n",
      "Epoch 00003: val_loss did not improve from 0.00012\n",
      "2110/2110 [==============================] - 15s 7ms/sample - loss: 2.2277e-04 - mean_absolute_error: 0.0101 - val_loss: 1.7763e-04 - val_mean_absolute_error: 0.0090\n",
      "Epoch 4/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 1.9897e-04 - mean_absolute_error: 0.0085\n",
      "Epoch 00004: val_loss improved from 0.00012 to 0.00006, saving model to results/2021-03-03_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-1-layers-2-units-256.h5\n",
      "2110/2110 [==============================] - 15s 7ms/sample - loss: 1.9877e-04 - mean_absolute_error: 0.0085 - val_loss: 5.6311e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 5/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.2766e-04 - mean_absolute_error: 0.0091\n",
      "Epoch 00005: val_loss did not improve from 0.00006\n",
      "2110/2110 [==============================] - 16s 7ms/sample - loss: 2.2895e-04 - mean_absolute_error: 0.0091 - val_loss: 9.7440e-05 - val_mean_absolute_error: 0.0069\n",
      "Epoch 6/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.3759e-04 - mean_absolute_error: 0.0094\n",
      "Epoch 00006: val_loss did not improve from 0.00006\n",
      "2110/2110 [==============================] - 16s 8ms/sample - loss: 2.3423e-04 - mean_absolute_error: 0.0095 - val_loss: 2.0610e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 7/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 3.1648e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00007: val_loss did not improve from 0.00006\n",
      "2110/2110 [==============================] - 15s 7ms/sample - loss: 3.1005e-04 - mean_absolute_error: 0.0126 - val_loss: 1.3116e-04 - val_mean_absolute_error: 0.0060\n",
      "Epoch 8/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.6419e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00008: val_loss did not improve from 0.00006\n",
      "2110/2110 [==============================] - 16s 7ms/sample - loss: 2.6858e-04 - mean_absolute_error: 0.0125 - val_loss: 9.7979e-05 - val_mean_absolute_error: 0.0079\n",
      "Epoch 9/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.4670e-04 - mean_absolute_error: 0.0100\n",
      "Epoch 00009: val_loss did not improve from 0.00006\n",
      "2110/2110 [==============================] - 15s 7ms/sample - loss: 2.5826e-04 - mean_absolute_error: 0.0100 - val_loss: 1.2642e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 10/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.1904e-04 - mean_absolute_error: 0.0092\n",
      "Epoch 00010: val_loss did not improve from 0.00006\n",
      "2110/2110 [==============================] - 17s 8ms/sample - loss: 2.1540e-04 - mean_absolute_error: 0.0091 - val_loss: 8.0685e-05 - val_mean_absolute_error: 0.0085\n",
      "Epoch 11/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.8118e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00011: val_loss did not improve from 0.00006\n",
      "2110/2110 [==============================] - 17s 8ms/sample - loss: 2.7516e-04 - mean_absolute_error: 0.0125 - val_loss: 1.2129e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 12/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 3.0375e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00012: val_loss did not improve from 0.00006\n",
      "2110/2110 [==============================] - 17s 8ms/sample - loss: 3.0928e-04 - mean_absolute_error: 0.0145 - val_loss: 6.1084e-05 - val_mean_absolute_error: 0.0075\n",
      "Epoch 13/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.4185e-04 - mean_absolute_error: 0.0098\n",
      "Epoch 00013: val_loss improved from 0.00006 to 0.00005, saving model to results/2021-03-03_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-1-layers-2-units-256.h5\n",
      "2110/2110 [==============================] - 17s 8ms/sample - loss: 2.4306e-04 - mean_absolute_error: 0.0098 - val_loss: 5.3860e-05 - val_mean_absolute_error: 0.0049\n",
      "Epoch 14/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 3.5943e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 00014: val_loss improved from 0.00005 to 0.00005, saving model to results/2021-03-03_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-1-layers-2-units-256.h5\n",
      "2110/2110 [==============================] - 17s 8ms/sample - loss: 3.5103e-04 - mean_absolute_error: 0.0134 - val_loss: 5.2684e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 15/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.9175e-04 - mean_absolute_error: 0.0119\n",
      "Epoch 00015: val_loss did not improve from 0.00005\n",
      "2110/2110 [==============================] - 16s 7ms/sample - loss: 2.9096e-04 - mean_absolute_error: 0.0120 - val_loss: 2.1863e-04 - val_mean_absolute_error: 0.0146\n",
      "Epoch 16/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.5204e-04 - mean_absolute_error: 0.0104\n",
      "Epoch 00016: val_loss did not improve from 0.00005\n",
      "2110/2110 [==============================] - 17s 8ms/sample - loss: 2.4869e-04 - mean_absolute_error: 0.0103 - val_loss: 5.3354e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 17/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.0613e-04 - mean_absolute_error: 0.0086\n",
      "Epoch 00017: val_loss did not improve from 0.00005\n",
      "2110/2110 [==============================] - 18s 9ms/sample - loss: 2.0488e-04 - mean_absolute_error: 0.0086 - val_loss: 6.5115e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 18/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.0359e-04 - mean_absolute_error: 0.0088\n",
      "Epoch 00018: val_loss did not improve from 0.00005\n",
      "2110/2110 [==============================] - 17s 8ms/sample - loss: 2.0726e-04 - mean_absolute_error: 0.0089 - val_loss: 1.0036e-04 - val_mean_absolute_error: 0.0093\n",
      "Epoch 19/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.3268e-04 - mean_absolute_error: 0.0113\n",
      "Epoch 00019: val_loss did not improve from 0.00005\n",
      "2110/2110 [==============================] - 15s 7ms/sample - loss: 2.3129e-04 - mean_absolute_error: 0.0113 - val_loss: 8.6155e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 20/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 1.6916e-04 - mean_absolute_error: 0.0079\n",
      "Epoch 00020: val_loss did not improve from 0.00005\n",
      "2110/2110 [==============================] - 15s 7ms/sample - loss: 1.8377e-04 - mean_absolute_error: 0.0081 - val_loss: 1.3586e-04 - val_mean_absolute_error: 0.0060\n"
     ]
    }
   ],
   "source": [
    "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# train the model and save the weights whenever we see \n",
    "# a new optimal model using ModelCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2110 samples, validate on 528 samples\n",
      "Epoch 1/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 0.0014 - mean_absolute_error: 0.0230\n",
      "Epoch 00001: val_loss improved from inf to 0.00005, saving model to results/2021-03-03_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-1-layers-2-units-256.h5\n",
      "2110/2110 [==============================] - 18s 9ms/sample - loss: 0.0014 - mean_absolute_error: 0.0228 - val_loss: 4.7701e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 2/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 3.9684e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 00002: val_loss did not improve from 0.00005\n",
      "2110/2110 [==============================] - 16s 8ms/sample - loss: 3.9020e-04 - mean_absolute_error: 0.0152 - val_loss: 1.1863e-04 - val_mean_absolute_error: 0.0050\n",
      "Epoch 3/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 3.2608e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00003: val_loss did not improve from 0.00005\n",
      "2110/2110 [==============================] - 17s 8ms/sample - loss: 3.1802e-04 - mean_absolute_error: 0.0130 - val_loss: 6.0142e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 4/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.5528e-04 - mean_absolute_error: 0.0097\n",
      "Epoch 00004: val_loss improved from 0.00005 to 0.00004, saving model to results/2021-03-03_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-1-layers-2-units-256.h5\n",
      "2110/2110 [==============================] - 16s 8ms/sample - loss: 2.4918e-04 - mean_absolute_error: 0.0096 - val_loss: 4.0064e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 5/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.3783e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00005: val_loss improved from 0.00004 to 0.00003, saving model to results/2021-03-03_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-1-layers-2-units-256.h5\n",
      "2110/2110 [==============================] - 16s 7ms/sample - loss: 2.3841e-04 - mean_absolute_error: 0.0090 - val_loss: 3.2818e-05 - val_mean_absolute_error: 0.0038\n",
      "Epoch 6/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.7222e-04 - mean_absolute_error: 0.0097\n",
      "Epoch 00006: val_loss did not improve from 0.00003\n",
      "2110/2110 [==============================] - 15s 7ms/sample - loss: 2.7556e-04 - mean_absolute_error: 0.0097 - val_loss: 5.7620e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 7/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.8623e-04 - mean_absolute_error: 0.0099\n",
      "Epoch 00007: val_loss did not improve from 0.00003\n",
      "2110/2110 [==============================] - 16s 8ms/sample - loss: 2.7968e-04 - mean_absolute_error: 0.0098 - val_loss: 1.9350e-04 - val_mean_absolute_error: 0.0075\n",
      "Epoch 8/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 3.0080e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00008: val_loss did not improve from 0.00003\n",
      "2110/2110 [==============================] - 15s 7ms/sample - loss: 3.0078e-04 - mean_absolute_error: 0.0122 - val_loss: 9.3528e-05 - val_mean_absolute_error: 0.0071\n",
      "Epoch 9/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.8788e-04 - mean_absolute_error: 0.0098\n",
      "Epoch 00009: val_loss did not improve from 0.00003\n",
      "2110/2110 [==============================] - 14s 7ms/sample - loss: 2.8549e-04 - mean_absolute_error: 0.0098 - val_loss: 1.0616e-04 - val_mean_absolute_error: 0.0062\n",
      "Epoch 10/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.1782e-04 - mean_absolute_error: 0.0101\n",
      "Epoch 00010: val_loss did not improve from 0.00003\n",
      "2110/2110 [==============================] - 15s 7ms/sample - loss: 2.2549e-04 - mean_absolute_error: 0.0103 - val_loss: 9.3734e-05 - val_mean_absolute_error: 0.0097\n",
      "Epoch 11/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.9153e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00011: val_loss did not improve from 0.00003\n",
      "2110/2110 [==============================] - 15s 7ms/sample - loss: 2.8954e-04 - mean_absolute_error: 0.0120 - val_loss: 1.5184e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 12/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.3933e-04 - mean_absolute_error: 0.0110\n",
      "Epoch 00012: val_loss did not improve from 0.00003\n",
      "2110/2110 [==============================] - 14s 7ms/sample - loss: 2.3701e-04 - mean_absolute_error: 0.0110 - val_loss: 1.1445e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 13/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.9725e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00013: val_loss did not improve from 0.00003\n",
      "2110/2110 [==============================] - 15s 7ms/sample - loss: 2.9038e-04 - mean_absolute_error: 0.0125 - val_loss: 3.7117e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 14/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 3.2734e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00014: val_loss did not improve from 0.00003\n",
      "2110/2110 [==============================] - 18s 8ms/sample - loss: 3.3683e-04 - mean_absolute_error: 0.0143 - val_loss: 3.4954e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 15/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 3.1813e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00015: val_loss did not improve from 0.00003\n",
      "2110/2110 [==============================] - 16s 8ms/sample - loss: 3.1928e-04 - mean_absolute_error: 0.0161 - val_loss: 8.2848e-05 - val_mean_absolute_error: 0.0091\n",
      "Epoch 16/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.8898e-04 - mean_absolute_error: 0.0105\n",
      "Epoch 00016: val_loss improved from 0.00003 to 0.00003, saving model to results/2021-03-03_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-1-layers-2-units-256.h5\n",
      "2110/2110 [==============================] - 15s 7ms/sample - loss: 2.8761e-04 - mean_absolute_error: 0.0105 - val_loss: 3.0166e-05 - val_mean_absolute_error: 0.0031\n",
      "Epoch 17/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.2824e-04 - mean_absolute_error: 0.0105\n",
      "Epoch 00017: val_loss did not improve from 0.00003\n",
      "2110/2110 [==============================] - 14s 7ms/sample - loss: 2.2962e-04 - mean_absolute_error: 0.0106 - val_loss: 6.3811e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 18/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 1.8454e-04 - mean_absolute_error: 0.0081\n",
      "Epoch 00018: val_loss did not improve from 0.00003\n",
      "2110/2110 [==============================] - 14s 7ms/sample - loss: 1.9730e-04 - mean_absolute_error: 0.0082 - val_loss: 4.1644e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 19/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.3591e-04 - mean_absolute_error: 0.0096\n",
      "Epoch 00019: val_loss did not improve from 0.00003\n",
      "2110/2110 [==============================] - 15s 7ms/sample - loss: 2.3123e-04 - mean_absolute_error: 0.0095 - val_loss: 3.5044e-05 - val_mean_absolute_error: 0.0044\n",
      "Epoch 20/20\n",
      "2048/2110 [============================>.] - ETA: 0s - loss: 2.1368e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 00020: val_loss did not improve from 0.00003\n",
      "2110/2110 [==============================] - 16s 7ms/sample - loss: 2.1256e-04 - mean_absolute_error: 0.0087 - val_loss: 1.8032e-04 - val_mean_absolute_error: 0.0065\n"
     ]
    }
   ],
   "source": [
    "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# train the model and save the weights whenever we see \n",
    "# a new optimal model using ModelCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-9-87357480c861>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-87357480c861>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tensorboard --logdir=\"logs\"\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "tensorboard --logdir=\"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph(test_df):\n",
    "    \"\"\"\n",
    "    This function plots true close price along with predicted close price\n",
    "    with blue and red colors respectively\n",
    "    \"\"\"\n",
    "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
    "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(model, data):\n",
    "    \"\"\"\n",
    "    This function takes the `model` and `data` dict to \n",
    "    construct a final dataframe that includes the features along \n",
    "    with true and predicted prices of the testing dataset\n",
    "    \"\"\"\n",
    "    # if predicted future price is higher than the current, \n",
    "    # then calculate the true future price minus the current price, to get the buy profit\n",
    "    buy_profit  = lambda current, true_future, pred_future: true_future - current if pred_future > current else 0\n",
    "    # if the predicted future price is lower than the current price,\n",
    "    # then subtract the true future price from the current price\n",
    "    sell_profit = lambda current, true_future, pred_future: current - true_future if pred_future < current else 0\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    # perform prediction and get prices\n",
    "    y_pred = model.predict(X_test)\n",
    "    if SCALE:\n",
    "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    test_df = data[\"test_df\"]\n",
    "    # add predicted future prices to the dataframe\n",
    "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
    "    # add true future prices to the dataframe\n",
    "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
    "    # sort the dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    # add the buy profit column\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    # add the sell profit column\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    if SCALE:\n",
    "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optimal model weights from results folder\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "if SCALE:\n",
    "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
    "else:\n",
    "    mean_absolute_error = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final dataframe for the testing set\n",
    "final_df = get_final_df(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the accuracy by counting the number of positive profits\n",
    "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
    "# calculating total buy & sell profit\n",
    "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
    "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
    "# total profit by adding sell & buy together\n",
    "total_profit = total_buy_profit + total_sell_profit\n",
    "# dividing total profit by number of testing samples (number of trades)\n",
    "profit_per_trade = total_profit / len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 1 days is 680.75$\n",
      "huber_loss loss: 3.016637743492152e-05\n",
      "Mean Absolute Error: 5.875603652636105\n",
      "Accuracy score: 0.4981060606060606\n",
      "Total buy profit: 64.53205680847168\n",
      "Total sell profit: 41.680999755859375\n",
      "Total profit: 106.21305656433105\n",
      "Profit per trade: 0.20116109197789972\n"
     ]
    }
   ],
   "source": [
    "# printing metrics\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Total buy profit:\", total_buy_profit)\n",
    "print(\"Total sell profit:\", total_sell_profit)\n",
    "print(\"Total profit:\", total_profit)\n",
    "print(\"Profit per trade:\", profit_per_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graph(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  open        high         low       close    adjclose  \\\n",
      "2020-12-08  625.510010  651.280029  618.500000  649.880005  649.880005   \n",
      "2020-12-28  674.510010  681.400024  660.799988  663.690002  663.690002   \n",
      "2021-01-08  856.000000  884.489990  838.390015  880.020020  880.020020   \n",
      "2021-01-13  852.760010  860.469971  832.000000  854.409973  854.409973   \n",
      "2021-01-19  837.799988  850.000000  833.000000  844.549988  844.549988   \n",
      "2021-01-22  834.309998  848.000000  828.619995  846.640015  846.640015   \n",
      "2021-02-04  855.000000  856.500000  833.419983  849.989990  849.989990   \n",
      "2021-02-05  845.000000  864.770020  838.969971  852.229980  852.229980   \n",
      "2021-02-23  662.130005  713.609985  619.000000  698.840027  698.840027   \n",
      "2021-03-01  690.109985  719.000000  685.049988  718.429993  718.429993   \n",
      "\n",
      "              volume ticker  adjclose_1  true_adjclose_1  buy_profit  \\\n",
      "2020-12-08  64265000   TSLA  605.604858       604.479980    0.000000   \n",
      "2020-12-28  32278600   TSLA  643.773132       665.989990  -19.916870   \n",
      "2021-01-08  75055500   TSLA  764.231689       811.190002    0.000000   \n",
      "2021-01-13  33312500   TSLA  812.510864       845.000000    0.000000   \n",
      "2021-01-19  25367000   TSLA  820.502319       850.450012  -24.047668   \n",
      "2021-01-22  20066500   TSLA  821.189514       880.799988  -25.450500   \n",
      "2021-02-04  15812700   TSLA  821.790222       852.229980  -28.199768   \n",
      "2021-02-05  18524800   TSLA  823.565308       863.419983  -28.664673   \n",
      "2021-02-23  66606900   TSLA  734.627563       742.020020   35.787537   \n",
      "2021-03-01  27009700   TSLA  688.803833       686.440002    0.000000   \n",
      "\n",
      "            sell_profit  \n",
      "2020-12-08    44.275146  \n",
      "2020-12-28     0.000000  \n",
      "2021-01-08   115.788330  \n",
      "2021-01-13    41.899109  \n",
      "2021-01-19     0.000000  \n",
      "2021-01-22     0.000000  \n",
      "2021-02-04     0.000000  \n",
      "2021-02-05     0.000000  \n",
      "2021-02-23     0.000000  \n",
      "2021-03-01    29.626160  \n"
     ]
    }
   ],
   "source": [
    "print(final_df.tail(10))\n",
    "# save the final dataframe to csv-results folder\n",
    "csv_results_folder = \"csv-results\"\n",
    "if not os.path.isdir(csv_results_folder):\n",
    "    os.mkdir(csv_results_folder)\n",
    "csv_filename = os.path.join(csv_results_folder, model_name + \".csv\")\n",
    "final_df.to_csv(csv_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
